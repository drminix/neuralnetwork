# Neural Networks and Deep Learning
## What is a neural network

- Deep learning refers "training very large neural network". 
- Each neuron performs certain operations like a multivariate linear regression(Y=WX+B) followed by a non-linear activation function operation such as ReLU
- Neural network is formed by stacking multiple layer of neurons.
Example neural network is shown below.
![alt text](img/simplenn.png)
- Input layer and hidden layer are density connected.  Every input feature is connected to every unit in the hidden layer.
- Rectified Linear Unit (ReLU) function. Rectify means taking a max of 0.

## Supervised learning
- In supervised learning, the training data you feed to the algorithm includes the desired solutions, called *labels*.
- Typical task of supervised learning: (1) classification such as spam filers and (2) regression
- Examples: (1) Linear regression, (2) Logistic Regression, (3) Support Vector Machines, Decision Trees and Random forests, (5) Neural networks
- Applications of Neural Network in supervised learning: Online Advertising, Photo tagging, Speech recognition, Machine translation, Autonomous driving
![alt text](img/supervisedlearning.png)
- Examples of neural networks
![alt text](img/neuralnetworkexamples.png)
- Structured Data: databases of data. Each of the feature has very well defined meaning 
- Unstructued Data: Audio, Image, Text. 
- Most of the successful application of DL has been in supervised learning.

## Unsupervised learning
- In unsupervised learning, the training data is unlabeled. The system tries to learn without a teacher
- Examples: (1) Clustering, Dimension Reduction, and etc. 

## Why Deep learning is taking off?
- Scale drives deep learning process. "Scale" means both (1) labled data & (2) size of NN.
- GPU + More Data(Digitization of data) + Algorithm advances in NN = Big Bang!
- One of the huge breakthrough in NN has been switching from SIGMOD function to ReLU function!. It's more computationally efficient!  Learning is slow in SIGMOD when gradient is nearly zero.
- Use of ReLU made the computation of gradient descent much faster.

## Logistic Regression as a Neural Network
### Binary Classification





 
